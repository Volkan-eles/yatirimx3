"""
Halkarz.com scraper - WordPress site yapÄ±sÄ±nÄ± kullanarak veri Ã§eker
"""
import requests
from bs4 import BeautifulSoup
import json
import re

def scrape_halkarz():
    """Ana scraper fonksiyonu"""
    
    print("=" * 70)
    print(" Halkarz.com Halka Arz Verileri Ã‡ekiliyor".center(70))
    print("=" * 70)
    
    active_ipos = []
    draft_ipos = []
    
    try:
        # 1. Ana sayfadan linkleri Ã§ek
        print("\n[1/3] Ana sayfa yÃ¼kleniyor...")
        url = "https://halkarz.com/"
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
        
        response = requests.get(url, headers=headers, timeout=20)
        response.raise_for_status()
        response.encoding = 'utf-8'
        
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # 2. Åirket linklerini bul
        print("[2/3] Halka arz linkleri aranÄ±yor...")
        
        # Halkarz.com'da ÅŸirketlerin linkleri genelde tek kelimelik path'lerde
        all_links = soup.find_all('a', href=True)
        
        company_links = []
        for link in all_links:
            href = link.get('href', '')
            
            # Åirket linklerini filtrele
            # Format: https://halkarz.com/ÅŸirket-adÄ±/ veya /ÅŸirket-adÄ±/
            if re.match(r'^https://halkarz\.com/[a-z0-9-]+/$', href) or \
               re.match(r'^/[a-z0-9-]+/$', href):
                
                # Gereksiz sayfalarÄ± Ã§Ä±kar
                ignore_list = [
                    '/halka-arz/', '/taslak-arzlar/', '/hakkimizda/',
                    '/iletisim/', '/gizlilik-politikasi/', '/cerez-politikasi/', 
                    '/kullanim-kosullari/', '/bist-100/', '/bist-30/'
                ]
                
                if href in ignore_list or href == '/':
                    continue
                
                # Tam URL'ye Ã§evir
                if href.startswith('/'):
                    href = 'https://halkarz.com' + href
                
                company_name = link.get_text(strip=True)
                
                if company_name and len(company_name) > 2:
                    company_links.append({
                        'url': href,
                        'name': company_name
                    })
        
        # Unique linkleri al
        seen = set()
        unique_links = []
        for link in company_links:
            if link['url'] not in seen:
                seen.add(link['url'])
                unique_links.append(link)
        
        print(f"   âœ“ {len(unique_links)} benzersiz ÅŸirket linki bulundu")
        
        # 3. Her ÅŸirket iÃ§in veri oluÅŸtur
        print("[3/3] Halka arz verileri oluÅŸturuluyor...")
        
        for i, link in enumerate(unique_links[:50], 1):  # Ä°lk 50 ile sÄ±nÄ±rla
            company_name = link['name']
            
            # Code'u ÅŸirket adÄ±ndan Ã§Ä±karmaya Ã§alÄ±ÅŸ
            code = ''
            if '(' in company_name and ')' in company_name:
                code = company_name[company_name.rfind('(')+1:company_name.rfind(')')].strip()
            
            ipo_data = {
                "code": code,
                "company": company_name,
                "dates": "",
                "status": "Taslak",
                "price": "Belirlenmedi",
                "lotCount": "Bilgi Yok",
                "distributionType": "BelirtilmemiÅŸ",
                "url": link['url']
            }
            
            # Aktif/taslak ayrÄ±mÄ± iÃ§in basit kontrol
            # "Talep" kelimesi iÃ§eriyorsa veya tarih var gibi gÃ¶rÃ¼nÃ¼yorsa aktif
            if 'talep' in company_name.lower() or re.search(r'\d{1,2}[-/]\d{1,2}', company_name):
                ipo_data['status'] = 'Talep ToplanÄ±yor'
                active_ipos.append(ipo_data)
            else:
                draft_ipos.append(ipo_data)
            
            if i % 10 == 0:
                print(f"   Ä°ÅŸlenen: {i}/{min(len(unique_links), 50)}")
        
        print(f"\nâœ… Toplam {len(active_ipos)} aktif, {len(draft_ipos)} taslak halka arz")
        
    except requests.RequestException as e:
        print(f"\nâŒ BaÄŸlantÄ± hatasÄ±: {e}")
        print("âš ï¸  Mevcut veriler korunuyor...")
        return load_existing_data()
    
    except Exception as e:
        print(f"\nâŒ Genel hata: {e}")
        import traceback
        traceback.print_exc()
        return load_existing_data()
    
    # SonuÃ§larÄ± kaydet
    result = {
        "active_ipos": active_ipos,
        "draft_ipos": draft_ipos
    }
    
    try:
        with open('public/halkarz_ipos.json', 'w', encoding='utf-8') as f:
            json.dump(result, f, ensure_ascii=False, indent=2)
        print(f"\nğŸ’¾ Veriler kaydedildi: public/halkarz_ipos.json")
    except Exception as e:
        print(f"\nâŒ KayÄ±t hatasÄ±: {e}")
    
    print("=" * 70)
    return result

def load_existing_data():
    """Mevcut veriyi yÃ¼kle"""
    try:
        with open('public/halkarz_ipos.json', 'r', encoding='utf-8') as f:
            return json.load(f)
    except:
        return {"active_ipos": [], "draft_ipos": []}

if __name__ == "__main__":
    scrape_halkarz()
